# Managing x86 jobs with SLURM

To do work on the VLSCI computers, you need to submit jobs to a queue. If resources are available then your job should run shortly afterwards, as long as you have quota available on your project. Whilst your jobs are running you will have the processors and memory dedicated to you that you requested for the amount of time that you have requested.d

Job scheduling, resource management and accounting are handled by [SLURM (Simple Linux Utility for Resource Management)](http://slurm.schedmd.com/).

**Caveat for parallel jobs**

__Note: for parallel jobs, use mpirun. SLURM normally uses srun, but there is an issue where jobs that use srun do not scale as well (are much slower).__

A typical workflow for managing a job is to:

* create a script that specifies the settings and commands to run the job and the resources required
* submit the job to the scheduler and resource manager
* monitor the job's progress
* modify the running job
* review the state of a finished job
* refine the script to run the job more efficiently

## Job script generator

To simplify the task of writing job submission scripts we provide an [interactive job script generator](https://www.vlsci.org.au/page/vlsci-job-script-generator-x86).

You can modify the script (or make your own) by referring to the Job types section (below).

## Translating PBS to SLURM

If you have PBS/Torque scripts, you might like to refer to [Comparison of PBS and SLURM script commands](https://www.vlsci.org.au/documentation/comparison-pbs-and-slurm-script-commands). We have also provided a software tool for converting PBS scripts to SLURM scripts called [pbs2slurm](https://www.vlsci.org.au/documentation/comparison-pbs-and-slurm-script-commands#translator).

## Job submission

To submit the resource requests to the queue, the `sbatch` command is used. In conjunction with the job-script, 
the command is of the following form:

```
sbatch [comand line options] jobname
```

This command will return a number for the job id of the job, e.g.:

```
Submitted batch job 94402
```

## Interactive jobs

It is possible to run a job as an interactive session using `sinteractive`.  For example:

```
sinteractive
```

or

```
sinteractive --x11
```

Will wait until your job runs, then give you a new promt from the node your job is on and the directory you launched the job from.  The `--x11` option will forward any X11 windows to your machine (assuming you have X11 and forwarding set up).

All SLURM options can be pased as options to the `sinteractive` command.

## Job monitoring

To view the state of the system use:

```
showq
```

or

```
squeue
```

To limit the output to your jobs only, use the form:

```
showq -u
```

or

```
squeue -u username
```

where `username` is your VLSCI username.

It is also possible to get more detailed information about a specific job using the `scontrol` 
command, see [The SLURM documentation for scontrol](http://slurm.schedmd.com/scontrol.html) for more information.

For example, to see the details of a job with job id jobID use:

```
scontrol show job jobID
```

To view SLURM accounting details use the `sacct` command, see [The SLURM documentation for sacct](http://slurm.schedmd.com/sacct.html) for more information.

For example:

```
sacct -j jobID
```

## Modifying a Job

If you notice that a job is taking longer that expected, please contact the Help Desk with the machine, job id and estimate of the extra time needed.

If you need to cancel your job, use:

```
scancel jobID
```

## Reviewing a Job

The best way to review a job is to view any output files that it generates and the output and error files generated by SLURM.  By default SLURM creates a file containing both the standard output and standard error.  This file is named as `slurm-jobID.out` (where jobID is the job id number).

To monitor and review SLURMs accounting information for the job use:

```
sacct
```

This is useful for queued, running and finished jobs.

## Optimising your script

An important step in job management it to customise the script to the job.  For example, it is always a good idea to do small test runs to estimate the total run time, best number of cores and memory requirements.  Underestimating can lead to jobs failing and overestimation will cause the job to remain in the queue longer than necessary and deprive other jobs from your project of quota.

At VLSCI there is a surcharge for memory, so it is important to monitor memory usage (e.g. with `sacct`).  Please see the page on Managing Memory.

## Job types

Jobs can be classed as one of three types: single CPU, SMP (or multithreaded), and MPI parallel. Here are some minimal examples for each type.

```
Single CPU
#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --time=01:00:00
#SBATCH --mem-per-cpu=2048
module load my-app-compiler/version
my-app
```

## Bundling single CPU jobs

For a large number of single jobs, it is better to bundle them with a wrapper. For example:

```
#!/bin/bash
#SBATCH --ntasks=16
#SBATCH --time=01:10:00
#SBATCH --mem-per-cpu=4096
for i in `seq 1 $SLURM_NTASKS`
do
    srun --nodes=1 --ntasks=1 --cpus-per-task=1 sh SINGLEJOB.slurm &
done
# IMPORTANT must wait for all to finish, or all get killed
wait
```

## SMP jobs (also called multithreaded, OpenMP)

```
#!/bin/bash
#SBATCH --time=01:00:00
#SBATCH --nodes=1
#SBATCH --exclusive
#SBATCH --mem=22528
module load my-app-compiler/version
my-app
```

## MPI Parallel Job

```
#!/bin/bash
#SBATCH --ntasks=16
#SBATCH --time=01:00:00
#SBATCH --mem-per-cpu=1024
module load my-app-compiler/version
mpirun my-MPI-app
```

Note: for parallel jobs, use mpirun.  SLURM normally uses srun, but there is an issue where jobs that use srun are slower than using mpirun.

## Software and Modules

Please see the [Software Applications](https://www.vlsci.org.au/documentation/software-applications) page for how to specify the software you need for the job.

## Common SLURM options 

<table border="1">
<tr><th>Option</th><th>Comment</th></tr>
<tr><td><code>#SBATCH --job-name=JobName</code></td><td>Give the job a name.</td></tr>
<tr><td><code>#SBATCH --account=VR9999</code></td><td>Account to charge quota.</td></tr>
<tr><td><code>#SBATCH -p main</code></td><td>Resources on a machine can be given a partition. <br> At VLSCI the default is called main.</td></tr>
<tr><td><code>#SBATCH --ntasks=128</code></td><td>The number of cores.</td></tr>
<tr><td><code>#SBATCH --mem-per-cpu=24576</code></td><td>Per core memory. Must be in MB.</td></tr>
<tr><td><code>#SBATCH --time=30-23:59:59</code></td><td>Walltime. Note <code>-</code> between days and hours.
<br>
Variants are:
<ul>
<li>minutes</li>
<li>minutes:seconds</li>
<li>hours:minutes:seconds</li>
<li>days-hours</li>
<li>days-hours:minutes</li>
<li>days-hours:minutes:seconds</li>
</td></tr>
<tr><td><code>#SBATCH --mail-type=FAIL</code></td><td>Send email notification when job fails</td></tr>
<tr><td><code>#SBATCH --mail-type=BEGIN</code></td><td>Send email notification when job start running</td></tr>
<tr><td><code>#SBATCH --mail-user name@email.address</code></td><td>E-mail address to send information to.
<br>
Best to not use this, and the system will use your known e-mail address.</td></tr>
<tr><td><code>#SBATCH -output path/file-%j.ext1</code></td><td>Redirect output to this file on the path (optional).
<br>
The name and extension can be anything you like. If you use <code>%j</code> then it is replaced by the job number.</td></tr>
<tr><td><code>#SBATCH -error path/file-%j.ext2</code></td><td>Redirect error to this file on the path (optional).<br>
The name and extension can be anything you like.If you use <code>%j</code> then it is replaced by the job number.</td></tr>
</table>

<p></p>

## SMP 

If your job is a SMP job, you will need to use the following options in addition to any relevant options from above.

To get the number of cpus that the node has, use:

```
$SLURM_CPUS_ON_NODE
```

<table border="1">
<tr><th>Option</th><th>Comment</th></tr>
<tr><td><code>#SBATCH --nodes=1
<br>
#SBATCH --exclusive
</code></td><td>Request exclusive use of 1 node. Do not use ntasks</td></tr>
</table>

<p></p>

## Job Arrays

If you use Job Arrays, please see the [SLURM documentation for job arrays](http://www.schedmd.com/slurmdocs/job_array.html).
